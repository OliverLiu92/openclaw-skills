#!/usr/bin/env python3
"""
ArXiv CS Weekly Paper Fetcher - å¢å¼ºç‰ˆ
æ¯å‘¨æŠ“å– arXiv CS æœ€æ–°è®ºæ–‡å¹¶æ•´ç†æˆä¸­æ–‡æŠ¥å‘Š
"""

import re
import sys
import json
import os
from datetime import datetime
from urllib.request import urlopen, Request
from urllib.error import URLError
from html.parser import HTMLParser

ARXIV_URL = "https://arxiv.org/list/cs/recent"
STATE_FILE = os.path.expanduser("~/.openclaw/workspace/.arxiv-cs-weekly-state.json")

def fetch_arxiv_page():
    """è·å– arXiv é¡µé¢å†…å®¹"""
    try:
        req = Request(
            ARXIV_URL,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
        )
        with urlopen(req, timeout=30) as response:
            return response.read().decode('utf-8')
    except URLError as e:
        print(f"Error fetching arXiv: {e}", file=sys.stderr)
        return None

def fetch_paper_abstract(arxiv_id):
    """è·å–å•ç¯‡è®ºæ–‡çš„æ‘˜è¦"""
    try:
        url = f"https://arxiv.org/abs/{arxiv_id}"
        req = Request(
            url,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
        )
        with urlopen(req, timeout=10) as response:
            html = response.read().decode('utf-8')
            # æå–æ‘˜è¦
            match = re.search(r'<blockquote[^>]*class="abstract mathjax"[^>]*>.*?<span[^>]*>Abstract:</span>(.*?)</blockquote>', html, re.DOTALL)
            if match:
                abstract = re.sub(r'<[^>]+>', '', match.group(1))
                return ' '.join(abstract.split())
    except Exception as e:
        print(f"Error fetching abstract for {arxiv_id}: {e}", file=sys.stderr)
    return None

class ArXivParser(HTMLParser):
    """è§£æ arXiv è®ºæ–‡åˆ—è¡¨"""
    
    def __init__(self):
        super().__init__()
        self.papers = []
        self.current_paper = {}
        self.in_item = False
        self.in_title = False
        self.in_authors = False
        self.in_comments = False
        self.in_subjects = False
        self.text_buffer = ""
        self.current_arxiv_id = None
        
    def handle_starttag(self, tag, attrs):
        attrs_dict = dict(attrs)
        
        # æ£€æµ‹è®ºæ–‡æ¡ç›® - ä» arXiv ID å¼€å§‹
        if tag == 'a' and 'href' in attrs_dict:
            href = attrs_dict['href']
            if '/abs/' in href:
                arxiv_id = href.split('/abs/')[-1]
                if arxiv_id and not self.in_item:
                    self.in_item = True
                    self.current_paper = {'arxiv_id': arxiv_id}
                    self.current_arxiv_id = arxiv_id
        
        # è®ºæ–‡æ ‡é¢˜
        if tag == 'div' and attrs_dict.get('class') == 'list-title mathjax':
            self.in_title = True
            self.text_buffer = ""
            
        # ä½œè€…
        if tag == 'div' and attrs_dict.get('class') == 'list-authors':
            self.in_authors = True
            self.text_buffer = ""
            
        # å­¦ç§‘åˆ†ç±»
        if tag == 'div' and attrs_dict.get('class') == 'list-subjects':
            self.in_subjects = True
            self.text_buffer = ""
            
    def handle_endtag(self, tag):
        if self.in_title and tag == 'div':
            self.in_title = False
            title = re.sub(r'^Title:\s*', '', self.text_buffer.strip())
            self.current_paper['title'] = title
            
        if self.in_authors and tag == 'div':
            self.in_authors = False
            # æå–çº¯æ–‡æœ¬ä½œè€…å
            authors_text = self.text_buffer.strip()
            authors_text = re.sub(r'^Authors:\s*', '', authors_text)
            authors_text = re.sub(r'<[^>]+>', '', authors_text)
            self.current_paper['authors'] = authors_text
            
        if self.in_subjects and tag == 'div':
            self.in_subjects = False
            subjects = re.sub(r'^Subjects:\s*', '', self.text_buffer.strip())
            self.current_paper['subjects'] = subjects
            
        # æ¡ç›®ç»“æŸ
        if tag == 'dd' and self.in_item and self.current_paper:
            if self.current_paper.get('title'):
                self.papers.append(self.current_paper)
            self.in_item = False
            self.current_paper = {}
            
    def handle_data(self, data):
        if self.in_title or self.in_authors or self.in_subjects:
            self.text_buffer += data

def parse_papers(html_content):
    """è§£æ HTML æå–è®ºæ–‡ä¿¡æ¯"""
    parser = ArXivParser()
    parser.feed(html_content)
    return parser.papers

def load_previous_state():
    """åŠ è½½ä¸Šæ¬¡è®°å½•çš„è®ºæ–‡ ID"""
    try:
        if os.path.exists(STATE_FILE):
            with open(STATE_FILE, 'r') as f:
                return json.load(f)
    except Exception as e:
        print(f"Error loading state: {e}", file=sys.stderr)
    return {'last_paper_ids': [], 'last_check_date': None}

def save_state(paper_ids):
    """ä¿å­˜å½“å‰è®ºæ–‡ ID åˆ—è¡¨"""
    try:
        os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)
        with open(STATE_FILE, 'w') as f:
            json.dump({
                'last_paper_ids': paper_ids,
                'last_check_date': datetime.now().isoformat()
            }, f)
    except Exception as e:
        print(f"Error saving state: {e}", file=sys.stderr)

def identify_new_papers(papers, previous_ids):
    """è¯†åˆ«æ–°è®ºæ–‡"""
    current_ids = [p.get('arxiv_id') for p in papers if p.get('arxiv_id')]
    new_papers = [p for p in papers if p.get('arxiv_id') and p.get('arxiv_id') not in previous_ids]
    return new_papers, current_ids

def simplify_subject(subjects_text):
    """ç®€åŒ–å­¦ç§‘åˆ†ç±»æ˜¾ç¤º"""
    if not subjects_text:
        return "è®¡ç®—æœºç§‘å­¦"
    # æå–æ‹¬å·å†…çš„çŸ­ä»£ç 
    codes = re.findall(r'\((cs\.[A-Z]+)\)', subjects_text)
    if codes:
        subject_map = {
            'cs.AI': 'äººå·¥æ™ºèƒ½',
            'cs.CL': 'è®¡ç®—è¯­è¨€å­¦/NLP',
            'cs.CV': 'è®¡ç®—æœºè§†è§‰',
            'cs.LG': 'æœºå™¨å­¦ä¹ ',
            'cs.RO': 'æœºå™¨äººå­¦',
            'cs.DB': 'æ•°æ®åº“',
            'cs.DC': 'åˆ†å¸ƒå¼è®¡ç®—',
            'cs.SE': 'è½¯ä»¶å·¥ç¨‹',
            'cs.CR': 'å¯†ç å­¦ä¸å®‰å…¨',
            'cs.HC': 'äººæœºäº¤äº’',
            'cs.IR': 'ä¿¡æ¯æ£€ç´¢',
            'cs.MM': 'å¤šåª’ä½“',
            'cs.NE': 'ç¥ç»ä¸è¿›åŒ–è®¡ç®—',
            'cs.OS': 'æ“ä½œç³»ç»Ÿ',
            'cs.PF': 'æ€§èƒ½è®¡ç®—',
            'cs.PL': 'ç¼–ç¨‹è¯­è¨€',
            'cs.SC': 'ç§‘å­¦è®¡ç®—',
            'cs.SY': 'ç³»ç»Ÿä¸æ§åˆ¶',
        }
        names = [subject_map.get(c, c) for c in codes[:2]]
        return 'ã€'.join(names) if names else subjects_text
    return subjects_text

def generate_simple_summary(title, abstract):
    """ç”Ÿæˆé€šä¿—åŒ–çš„é—®é¢˜æè¿°"""
    if not abstract:
        return "æš‚æ— è¯¦ç»†æ‘˜è¦"
    
    # ç®€åŒ–æ‘˜è¦ï¼Œæå–æ ¸å¿ƒé—®é¢˜
    # é€šå¸¸å‰ä¸¤å¥åŒ…å«æ ¸å¿ƒè´¡çŒ®
    sentences = abstract.split('. ')
    if len(sentences) >= 2:
        summary = '. '.join(sentences[:2]) + '.'
    else:
        summary = abstract[:300] + '...' if len(abstract) > 300 else abstract
    
    return summary

def generate_report(papers, previous_ids=None, max_papers=15, fetch_abstracts=False):
    """ç”Ÿæˆä¸­æ–‡æŠ¥å‘Š"""
    if not papers:
        return "æœªæ‰¾åˆ°è®ºæ–‡æ•°æ®ã€‚"
    
    # è¯†åˆ«æ–°è®ºæ–‡
    if previous_ids:
        new_papers, current_ids = identify_new_papers(papers, previous_ids)
        is_new_mark = lambda pid: "ğŸ†• " if pid not in previous_ids else ""
    else:
        new_papers = papers
        current_ids = [p.get('arxiv_id') for p in papers]
        is_new_mark = lambda pid: ""
    
    now = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    report = f"""# ğŸ“š arXiv CS æœ€æ–°è®ºæ–‡å‘¨æŠ¥

> **æŠ¥å‘Šæ—¶é—´**ï¼š{now}  
> **æ¥æº**ï¼š[arXiv CS Recent](https://arxiv.org/list/cs/recent)  
> **æœ¬å‘¨è®ºæ–‡æ€»æ•°**ï¼š{len(papers)} ç¯‡  
> **æ–°å¢è®ºæ–‡**ï¼š{len(new_papers)} ç¯‡  
> **ä»¥ä¸‹å±•ç¤ºå‰ {min(max_papers, len(papers))} ç¯‡**

---

"""
    
    for i, paper in enumerate(papers[:max_papers], 1):
        title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
        authors = paper.get('authors', 'æœªçŸ¥ä½œè€…')
        arxiv_id = paper.get('arxiv_id', '')
        subjects = simplify_subject(paper.get('subjects', ''))
        
        # ç®€åŒ–ä½œè€…åˆ—è¡¨
        author_list = [a.strip() for a in authors.split(',') if a.strip()]
        if len(author_list) > 3:
            authors_short = f"{', '.join(author_list[:3])} ç­‰ {len(author_list)} ä½ä½œè€…"
        else:
            authors_short = authors
        
        # è·å–æ‘˜è¦
        abstract = paper.get('abstract', '')
        if not abstract and fetch_abstracts and arxiv_id:
            print(f"æ­£åœ¨è·å– {arxiv_id} çš„æ‘˜è¦...", file=sys.stderr)
            abstract = fetch_paper_abstract(arxiv_id) or "æš‚æ— æ‘˜è¦"
        
        summary = generate_simple_summary(title, abstract) if abstract else "æš‚æ— æ‘˜è¦"
        new_mark = is_new_mark(arxiv_id)
        
        report += f"""## {i}. {new_mark}{title}

**ä½œè€…**ï¼š{authors_short}  
**arXiv ID**ï¼š{arxiv_id}  
**é¢†åŸŸ**ï¼š{subjects}  
**æ ¸å¿ƒé—®é¢˜**ï¼š{summary}

---

"""
    
    # æ·»åŠ é¡µè„š
    report += """## ğŸ“Š æ€»ç»“

æœ¬å‘¨ arXiv CS é¢†åŸŸå…±æœ‰ **""" + str(len(papers)) + """** ç¯‡æ–°è®ºæ–‡ï¼Œæ¶µç›– """ + subjects + """ ç­‰æ–¹å‘ã€‚

---

*æŠ¥å‘Šç”± OpenClaw arxiv-cs-weekly Skill è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    return report, current_ids

def main():
    """ä¸»å‡½æ•°"""
    print("æ­£åœ¨è·å– arXiv CS æœ€æ–°è®ºæ–‡...", file=sys.stderr)
    
    # åŠ è½½ä¸Šæ¬¡çŠ¶æ€
    state = load_previous_state()
    previous_ids = state.get('last_paper_ids', [])
    last_check = state.get('last_check_date')
    
    if last_check:
        print(f"ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´: {last_check}", file=sys.stderr)
    
    html_content = fetch_arxiv_page()
    if not html_content:
        print("è·å–å¤±è´¥", file=sys.stderr)
        sys.exit(1)
    
    papers = parse_papers(html_content)
    print(f"æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡", file=sys.stderr)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è·å–è¯¦ç»†æ‘˜è¦ï¼ˆå‘½ä»¤è¡Œå‚æ•° --fullï¼‰
    fetch_abstracts = '--full' in sys.argv
    
    report, current_ids = generate_report(papers, previous_ids, max_papers=15, fetch_abstracts=fetch_abstracts)
    print(report)
    
    # ä¿å­˜çŠ¶æ€
    save_state(current_ids)
    print(f"\nå·²ä¿å­˜ {len(current_ids)} ç¯‡è®ºæ–‡ ID åˆ°çŠ¶æ€æ–‡ä»¶", file=sys.stderr)

if __name__ == "__main__":
    main()
