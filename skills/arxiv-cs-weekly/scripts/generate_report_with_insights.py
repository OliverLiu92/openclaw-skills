#!/usr/bin/env python3
"""
ArXiv CS Weekly - å¸¦æ·±åº¦è§£è¯»çš„æŠ¥å‘Šç”Ÿæˆå™¨
ç”Ÿæˆé€‚åˆå¤§è±¡å‘é€çš„æ ¼å¼åŒ–æŠ¥å‘Šï¼ŒåŒ…å«è®ºæ–‡åŸºç¡€ä¿¡æ¯+è§£è¯»è¯·æ±‚æ ‡è®°
"""

import re
import json
import os
from datetime import datetime
from urllib.request import urlopen, Request
from html.parser import HTMLParser

ARXIV_URL = "https://arxiv.org/list/cs/recent"
STATE_FILE = os.path.expanduser("~/.openclaw/workspace/.arxiv-cs-weekly-state.json")

class ArXivParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.papers = []
        self.current_paper = {}
        self.in_item = False
        self.in_title = False
        self.in_authors = False
        self.in_subjects = False
        self.text_buffer = ""
        
    def handle_starttag(self, tag, attrs):
        attrs_dict = dict(attrs)
        if tag == 'a' and 'href' in attrs_dict:
            href = attrs_dict['href']
            if '/abs/' in href:
                arxiv_id = href.split('/abs/')[-1]
                if arxiv_id and not self.in_item:
                    self.in_item = True
                    self.current_paper = {'arxiv_id': arxiv_id}
        
        if tag == 'div' and attrs_dict.get('class') == 'list-title mathjax':
            self.in_title = True
            self.text_buffer = ""
        if tag == 'div' and attrs_dict.get('class') == 'list-authors':
            self.in_authors = True
            self.text_buffer = ""
        if tag == 'div' and attrs_dict.get('class') == 'list-subjects':
            self.in_subjects = True
            self.text_buffer = ""
            
    def handle_endtag(self, tag):
        if self.in_title and tag == 'div':
            self.in_title = False
            self.current_paper['title'] = re.sub(r'^Title:\s*', '', self.text_buffer.strip())
        if self.in_authors and tag == 'div':
            self.in_authors = False
            authors = re.sub(r'^Authors:\s*', '', self.text_buffer.strip())
            self.current_paper['authors'] = re.sub(r'<[^>]+>', '', authors)
        if self.in_subjects and tag == 'div':
            self.in_subjects = False
            self.current_paper['subjects'] = re.sub(r'^Subjects:\s*', '', self.text_buffer.strip())
        if tag == 'dd' and self.in_item and self.current_paper:
            if self.current_paper.get('title'):
                self.papers.append(self.current_paper)
            self.in_item = False
            self.current_paper = {}
            
    def handle_data(self, data):
        if self.in_title or self.in_authors or self.in_subjects:
            self.text_buffer += data

def fetch_page():
    try:
        req = Request(ARXIV_URL, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'})
        with urlopen(req, timeout=30) as response:
            return response.read().decode('utf-8')
    except Exception as e:
        print(f"Error: {e}", file=os.sys.stderr)
        return None

def load_state():
    try:
        if os.path.exists(STATE_FILE):
            with open(STATE_FILE, 'r') as f:
                return json.load(f)
    except:
        pass
    return {'last_paper_ids': [], 'last_check_date': None}

def save_state(paper_ids):
    try:
        os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)
        with open(STATE_FILE, 'w') as f:
            json.dump({'last_paper_ids': paper_ids, 'last_check_date': datetime.now().isoformat()}, f)
    except:
        pass

def simplify_subject(text):
    if not text:
        return "CS"
    codes = re.findall(r'\((cs\.[A-Z]+)\)', text)
    if codes:
        mapping = {
            'cs.AI': 'AI', 'cs.CL': 'NLP', 'cs.CV': 'CV',
            'cs.LG': 'ML', 'cs.RO': 'æœºå™¨äºº', 'cs.DB': 'æ•°æ®åº“',
            'cs.SE': 'è½¯ä»¶å·¥ç¨‹', 'cs.CR': 'å®‰å…¨', 'cs.IR': 'IR',
            'cs.MM': 'å¤šåª’ä½“', 'cs.DC': 'åˆ†å¸ƒå¼', 'cs.OS': 'ç³»ç»Ÿ',
            'cs.PL': 'ç¼–ç¨‹è¯­è¨€', 'cs.SC': 'ç§‘å­¦è®¡ç®—'
        }
        names = [mapping.get(c, c) for c in codes[:2]]
        return 'ã€'.join(names)
    return text

def get_keywords(title):
    keywords = []
    t = title.lower()
    maps = {
        'llm': 'LLM', 'language model': 'LLM', 'transformer': 'Transformer',
        'diffusion': 'æ‰©æ•£', 'vision': 'è§†è§‰', 'image': 'å›¾åƒ',
        'video': 'è§†é¢‘', 'robot': 'æœºå™¨äºº', 'reinforcement': 'RL',
        'multimodal': 'å¤šæ¨¡æ€', '3d': '3D', 'optimization': 'ä¼˜åŒ–',
        'attention': 'æ³¨æ„åŠ›', 'embedding': 'Embedding', 'rag': 'RAG',
        'agent': 'Agent', 'prompt': 'Prompt', 'fine-tuning': 'å¾®è°ƒ',
        'zero-shot': 'é›¶æ ·æœ¬', 'chain-of-thought': 'CoT',
        'test-time': 'TTT', 'training': 'è®­ç»ƒ', 'inference': 'æ¨ç†',
        'memory': 'å†…å­˜', 'efficient': 'é«˜æ•ˆ', 'parallel': 'å¹¶è¡Œ',
        'binding': 'ç»‘å®š', 'key-value': 'KV', 'linear': 'çº¿æ€§'
    }
    for k, v in maps.items():
        if k in t and v not in keywords:
            keywords.append(v)
    return keywords[:3]

def generate_report_with_insights():
    """ç”ŸæˆæŠ¥å‘Šï¼ŒåŒ…å«è§£è¯»è¯·æ±‚æ ‡è®°"""
    html = fetch_page()
    if not html:
        return None, []
    
    parser = ArXivParser()
    parser.feed(html)
    papers = parser.papers
    
    state = load_state()
    prev_ids = state.get('last_paper_ids', [])
    
    current_ids = [p.get('arxiv_id') for p in papers if p.get('arxiv_id')]
    new_papers = [p for p in papers if p.get('arxiv_id') and p.get('arxiv_id') not in prev_ids]
    
    now = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    lines = [
        f"ğŸ“š **ArXiv CS è®ºæ–‡å‘¨æŠ¥** ({now})",
        "",
        f"æœ¬å‘¨æ–°å¢ *{len(papers)}* ç¯‡ï¼ŒğŸ†• å…¨æ–° *{len(new_papers)}* ç¯‡",
        "",
        "---",
        ""
    ]
    
    # åªé€‰å‰5ç¯‡åšè¯¦ç»†è§£è¯»ï¼Œå…¶ä½™ç®€å•åˆ—å‡º
    detailed_count = min(5, len(papers))
    
    for i, p in enumerate(papers[:detailed_count], 1):
        title = p.get('title', 'æœªçŸ¥')
        authors = p.get('authors', 'æœªçŸ¥')
        aid = p.get('arxiv_id', '')
        subj = simplify_subject(p.get('subjects', ''))
        
        alist = [a.strip() for a in authors.split(',') if a.strip()]
        if len(alist) > 2:
            authors_short = f"{', '.join(alist[:2])} ç­‰{len(alist)}äºº"
        else:
            authors_short = authors
        
        kws = get_keywords(title)
        kw_str = f"ã€Œ{' Â· '.join(kws)}ã€" if kws else ""
        is_new = "ğŸ†• " if aid not in prev_ids else ""
        
        lines.append(f"### {i}. {is_new}{title}")
        lines.append(f"{kw_str}")
        lines.append(f"ğŸ‘¤ {authors_short} | ğŸ·ï¸ {subj}")
        lines.append(f"ğŸ”— https://arxiv.org/abs/{aid}")
        lines.append("")
        lines.append(f"ğŸ’¡ **è§£è¯»è¯·æ±‚**: è¯·ä½¿ç”¨ paper-insights-deep skill è§£è¯» arXiv:{aid}")
        lines.append("")
        lines.append("---")
        lines.append("")
    
    # å…¶ä»–è®ºæ–‡ç®€å•åˆ—å‡º
    if len(papers) > detailed_count:
        lines.append("### ğŸ“ å…¶ä»–è®ºæ–‡")
        lines.append("")
        for i, p in enumerate(papers[detailed_count:detailed_count+5], detailed_count+1):
            title = p.get('title', 'æœªçŸ¥')[:60] + "..." if len(p.get('title', '')) > 60 else p.get('title', 'æœªçŸ¥')
            aid = p.get('arxiv_id', '')
            subj = simplify_subject(p.get('subjects', ''))
            is_new = "ğŸ†• " if aid not in prev_ids else ""
            lines.append(f"{i}. {is_new}*{title}* ({subj}) - [é“¾æ¥](https://arxiv.org/abs/{aid})")
        lines.append("")
        if len(papers) > detailed_count + 5:
            lines.append(f"...è¿˜æœ‰ {len(papers) - detailed_count - 5} ç¯‡")
        lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("ğŸ¤– *OpenClaw è‡ªåŠ¨ç”Ÿæˆ* | è¯¦ç»†è§£è¯»è¯·å›å¤å¯¹åº”è®ºæ–‡ç¼–å·")
    
    save_state(current_ids)
    return "\n".join(lines), papers[:detailed_count]

if __name__ == "__main__":
    report, detailed_papers = generate_report_with_insights()
    if report:
        print(report)
        # è¾“å‡ºéœ€è¦è§£è¯»çš„è®ºæ–‡IDåˆ—è¡¨ï¼ˆä¾›åç»­å¤„ç†ï¼‰
        if detailed_papers:
            print("\n" + "="*60, file=os.sys.stderr)
            print("ä»¥ä¸‹è®ºæ–‡éœ€è¦æ·±åº¦è§£è¯»:", file=os.sys.stderr)
            for p in detailed_papers:
                print(f"  - arXiv:{p.get('arxiv_id')} - {p.get('title', '')[:50]}...", file=os.sys.stderr)
            print("="*60, file=os.sys.stderr)
    else:
        print("è·å–è®ºæ–‡å¤±è´¥")
        exit(1)
