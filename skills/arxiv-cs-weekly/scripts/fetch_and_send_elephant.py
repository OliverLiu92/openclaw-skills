#!/usr/bin/env python3
"""
ArXiv CS Weekly Paper Fetcher - å¤§è±¡æ’ä»¶å‘é€ç‰ˆ
æ¯å‘¨æŠ“å– arXiv CS æœ€æ–°è®ºæ–‡å¹¶é€šè¿‡å¤§è±¡æ’ä»¶å‘é€
"""

import re
import sys
import json
import os
import subprocess
from datetime import datetime
from urllib.request import urlopen, Request
from urllib.error import URLError
from html.parser import HTMLParser

ARXIV_URL = "https://arxiv.org/list/cs/recent"
STATE_FILE = os.path.expanduser("~/.openclaw/workspace/.arxiv-cs-weekly-state.json")

def fetch_arxiv_page():
    """è·å– arXiv é¡µé¢å†…å®¹"""
    try:
        req = Request(
            ARXIV_URL,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
        )
        with urlopen(req, timeout=30) as response:
            return response.read().decode('utf-8')
    except URLError as e:
        print(f"Error fetching arXiv: {e}", file=sys.stderr)
        return None

class ArXivParser(HTMLParser):
    """è§£æ arXiv è®ºæ–‡åˆ—è¡¨"""
    
    def __init__(self):
        super().__init__()
        self.papers = []
        self.current_paper = {}
        self.in_item = False
        self.in_title = False
        self.in_authors = False
        self.in_subjects = False
        self.text_buffer = ""
        self.current_arxiv_id = None
        
    def handle_starttag(self, tag, attrs):
        attrs_dict = dict(attrs)
        
        if tag == 'a' and 'href' in attrs_dict:
            href = attrs_dict['href']
            if '/abs/' in href:
                arxiv_id = href.split('/abs/')[-1]
                if arxiv_id and not self.in_item:
                    self.in_item = True
                    self.current_paper = {'arxiv_id': arxiv_id}
                    self.current_arxiv_id = arxiv_id
        
        if tag == 'div' and attrs_dict.get('class') == 'list-title mathjax':
            self.in_title = True
            self.text_buffer = ""
            
        if tag == 'div' and attrs_dict.get('class') == 'list-authors':
            self.in_authors = True
            self.text_buffer = ""
            
        if tag == 'div' and attrs_dict.get('class') == 'list-subjects':
            self.in_subjects = True
            self.text_buffer = ""
            
    def handle_endtag(self, tag):
        if self.in_title and tag == 'div':
            self.in_title = False
            title = re.sub(r'^Title:\s*', '', self.text_buffer.strip())
            self.current_paper['title'] = title
            
        if self.in_authors and tag == 'div':
            self.in_authors = False
            authors_text = self.text_buffer.strip()
            authors_text = re.sub(r'^Authors:\s*', '', authors_text)
            authors_text = re.sub(r'<[^>]+>', '', authors_text)
            self.current_paper['authors'] = authors_text
            
        if self.in_subjects and tag == 'div':
            self.in_subjects = False
            subjects = re.sub(r'^Subjects:\s*', '', self.text_buffer.strip())
            self.current_paper['subjects'] = subjects
            
        if tag == 'dd' and self.in_item and self.current_paper:
            if self.current_paper.get('title'):
                self.papers.append(self.current_paper)
            self.in_item = False
            self.current_paper = {}
            
    def handle_data(self, data):
        if self.in_title or self.in_authors or self.in_subjects:
            self.text_buffer += data

def parse_papers(html_content):
    parser = ArXivParser()
    parser.feed(html_content)
    return parser.papers

def load_previous_state():
    try:
        if os.path.exists(STATE_FILE):
            with open(STATE_FILE, 'r') as f:
                return json.load(f)
    except Exception as e:
        print(f"Error loading state: {e}", file=sys.stderr)
    return {'last_paper_ids': [], 'last_check_date': None}

def save_state(paper_ids):
    try:
        os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)
        with open(STATE_FILE, 'w') as f:
            json.dump({
                'last_paper_ids': paper_ids,
                'last_check_date': datetime.now().isoformat()
            }, f)
    except Exception as e:
        print(f"Error saving state: {e}", file=sys.stderr)

def identify_new_papers(papers, previous_ids):
    current_ids = [p.get('arxiv_id') for p in papers if p.get('arxiv_id')]
    new_papers = [p for p in papers if p.get('arxiv_id') and p.get('arxiv_id') not in previous_ids]
    return new_papers, current_ids

def simplify_subject(subjects_text):
    if not subjects_text:
        return "è®¡ç®—æœºç§‘å­¦"
    codes = re.findall(r'\((cs\.[A-Z]+)\)', subjects_text)
    if codes:
        subject_map = {
            'cs.AI': 'äººå·¥æ™ºèƒ½',
            'cs.CL': 'è®¡ç®—è¯­è¨€å­¦/NLP',
            'cs.CV': 'è®¡ç®—æœºè§†è§‰',
            'cs.LG': 'æœºå™¨å­¦ä¹ ',
            'cs.RO': 'æœºå™¨äººå­¦',
            'cs.DB': 'æ•°æ®åº“',
            'cs.DC': 'åˆ†å¸ƒå¼è®¡ç®—',
            'cs.SE': 'è½¯ä»¶å·¥ç¨‹',
            'cs.CR': 'å¯†ç å­¦ä¸å®‰å…¨',
            'cs.HC': 'äººæœºäº¤äº’',
            'cs.IR': 'ä¿¡æ¯æ£€ç´¢',
            'cs.MM': 'å¤šåª’ä½“',
            'cs.NE': 'ç¥ç»ä¸è¿›åŒ–è®¡ç®—',
            'cs.OS': 'æ“ä½œç³»ç»Ÿ',
            'cs.PF': 'æ€§èƒ½è®¡ç®—',
            'cs.PL': 'ç¼–ç¨‹è¯­è¨€',
            'cs.SC': 'ç§‘å­¦è®¡ç®—',
            'cs.SY': 'ç³»ç»Ÿä¸æ§åˆ¶',
        }
        names = [subject_map.get(c, c) for c in codes[:2]]
        return 'ã€'.join(names) if names else subjects_text
    return subjects_text

def generate_simple_summary(title):
    """ä»æ ‡é¢˜ç”Ÿæˆé€šä¿—åŒ–æè¿°"""
    # å°è¯•æå–å…³é”®ä¿¡æ¯
    keywords = []
    
    # å¸¸è§å…³é”®è¯æ˜ å°„
    keyword_map = {
        'llm': 'å¤§è¯­è¨€æ¨¡å‹',
        'large language model': 'å¤§è¯­è¨€æ¨¡å‹',
        'transformer': 'Transformeræ¶æ„',
        'diffusion': 'æ‰©æ•£æ¨¡å‹',
        'gpt': 'GPTæ¨¡å‹',
        'bert': 'BERTæ¨¡å‹',
        'vision': 'è§†è§‰',
        'image': 'å›¾åƒ',
        'video': 'è§†é¢‘',
        'robot': 'æœºå™¨äºº',
        'reinforcement': 'å¼ºåŒ–å­¦ä¹ ',
        'fine-tuning': 'å¾®è°ƒ',
        'pre-training': 'é¢„è®­ç»ƒ',
        'multimodal': 'å¤šæ¨¡æ€',
        '3d': '3D',
        'generation': 'ç”Ÿæˆ',
        'understanding': 'ç†è§£',
        'reasoning': 'æ¨ç†',
        'planning': 'è§„åˆ’',
        'segmentation': 'åˆ†å‰²',
        'detection': 'æ£€æµ‹',
        'classification': 'åˆ†ç±»',
        'optimization': 'ä¼˜åŒ–',
        'efficient': 'é«˜æ•ˆ',
        'memory': 'å†…å­˜',
        'training': 'è®­ç»ƒ',
        'inference': 'æ¨ç†',
        'sampling': 'é‡‡æ ·',
        'attention': 'æ³¨æ„åŠ›æœºåˆ¶',
        'embedding': 'åµŒå…¥',
        'retrieval': 'æ£€ç´¢',
        'indexing': 'ç´¢å¼•',
        'compression': 'å‹ç¼©',
        'distillation': 'çŸ¥è¯†è’¸é¦',
        'pruning': 'å‰ªæ',
        'quantization': 'é‡åŒ–',
        'federated': 'è”é‚¦å­¦ä¹ ',
        'adversarial': 'å¯¹æŠ—å­¦ä¹ ',
        'self-supervised': 'è‡ªç›‘ç£',
        'contrastive': 'å¯¹æ¯”å­¦ä¹ ',
        'zero-shot': 'é›¶æ ·æœ¬',
        'few-shot': 'å°‘æ ·æœ¬',
        'prompt': 'æç¤ºè¯',
        'chain-of-thought': 'æ€ç»´é“¾',
        'rag': 'RAGæ£€ç´¢å¢å¼º',
        'agent': 'æ™ºèƒ½ä½“',
    }
    
    title_lower = title.lower()
    for en, cn in keyword_map.items():
        if en in title_lower and cn not in keywords:
            keywords.append(cn)
    
    if keywords:
        return f"ç ”ç©¶{'ã€'.join(keywords[:3])}ç›¸å…³é—®é¢˜"
    return "æ¢ç´¢è®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„æ–°æ–¹æ³•"

def format_paper_for_elephant(paper, index, is_new=False):
    """æ ¼å¼åŒ–å•ç¯‡è®ºæ–‡ä¸ºå¤§è±¡æ¶ˆæ¯æ ¼å¼"""
    title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
    authors = paper.get('authors', 'æœªçŸ¥ä½œè€…')
    arxiv_id = paper.get('arxiv_id', '')
    subjects = simplify_subject(paper.get('subjects', ''))
    
    # ç®€åŒ–ä½œè€…åˆ—è¡¨
    author_list = [a.strip() for a in authors.split(',') if a.strip()]
    if len(author_list) > 3:
        authors_short = f"{', '.join(author_list[:3])} ç­‰{len(author_list)}äºº"
    else:
        authors_short = authors
    
    summary = generate_simple_summary(title)
    new_mark = "ğŸ†• " if is_new else ""
    
    return f"""{index}. {new_mark}*{title}*
   ğŸ‘¤ {authors_short}
   ğŸ·ï¸ {subjects}
   ğŸ’¡ {summary}
   ğŸ”— https://arxiv.org/abs/{arxiv_id}
"""

def generate_elephant_message(papers, new_papers_count, total_count):
    """ç”Ÿæˆå¤§è±¡æ¶ˆæ¯æ ¼å¼"""
    now = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    # å–å‰10ç¯‡
    display_papers = papers[:10]
    
    message = f"""ğŸ“š **ArXiv CS è®ºæ–‡å‘¨æŠ¥** ({now})

æœ¬å‘¨æ–°å¢ *{total_count}* ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ ğŸ†• *{new_papers_count}* ç¯‡ä¸ºå…¨æ–°å‘å¸ƒ

---

"""
    
    # åŠ è½½ä¸Šæ¬¡çŠ¶æ€ä»¥æ ‡è®°æ–°è®ºæ–‡
    state = load_previous_state()
    previous_ids = state.get('last_paper_ids', [])
    
    for i, paper in enumerate(display_papers, 1):
        is_new = paper.get('arxiv_id') not in previous_ids
        message += format_paper_for_elephant(paper, i, is_new) + "\n"
    
    if len(papers) > 10:
        message += f"\n...è¿˜æœ‰ {len(papers) - 10} ç¯‡è®ºæ–‡ï¼ŒæŸ¥çœ‹å®Œæ•´åˆ—è¡¨ï¼šhttps://arxiv.org/list/cs/recent\n"
    
    message += """
---
ğŸ¤– *ç”± OpenClaw è‡ªåŠ¨ç”Ÿæˆ*"""
    
    return message

def send_to_elephant(message):
    """é€šè¿‡å¤§è±¡æ’ä»¶å‘é€æ¶ˆæ¯ - ä½¿ç”¨ cron çš„ announce æ¨¡å¼è‡ªåŠ¨å‘é€"""
    # å®é™…å‘é€ç”± OpenClaw cron ä»»åŠ¡çš„ --announce å‚æ•°å¤„ç†
    # è¿™é‡Œåªæ‰“å°æ¶ˆæ¯å†…å®¹ä¾› cron æ•è·
    print("\n" + "="*60, file=sys.stderr)
    print("ğŸ“± å¤§è±¡æ¶ˆæ¯å†…å®¹å·²ç”Ÿæˆï¼ˆå°†ç”± cron ä»»åŠ¡è‡ªåŠ¨å‘é€ï¼‰", file=sys.stderr)
    print("="*60 + "\n", file=sys.stderr)
    return True

def main():
    print("æ­£åœ¨è·å– arXiv CS æœ€æ–°è®ºæ–‡...", file=sys.stderr)
    
    state = load_previous_state()
    previous_ids = state.get('last_paper_ids', [])
    
    html_content = fetch_arxiv_page()
    if not html_content:
        print("è·å–å¤±è´¥", file=sys.stderr)
        sys.exit(1)
    
    papers = parse_papers(html_content)
    print(f"æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡", file=sys.stderr)
    
    # è¯†åˆ«æ–°è®ºæ–‡
    new_papers, current_ids = identify_new_papers(papers, previous_ids)
    
    # ç”Ÿæˆå¤§è±¡æ¶ˆæ¯
    message = generate_elephant_message(papers, len(new_papers), len(papers))
    
    # æ‰“å°åˆ° stdoutï¼ˆç”¨äºè°ƒè¯•ï¼‰
    print(message)
    
    # å‘é€åˆ°å¤§è±¡
    send_to_elephant(message)
    
    # ä¿å­˜çŠ¶æ€
    save_state(current_ids)
    print(f"å·²ä¿å­˜ {len(current_ids)} ç¯‡è®ºæ–‡ ID", file=sys.stderr)

if __name__ == "__main__":
    main()
