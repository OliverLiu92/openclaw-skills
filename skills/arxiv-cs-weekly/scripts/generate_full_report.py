#!/usr/bin/env python3
"""
ArXiv CS Weekly - å®Œæ•´ç‰ˆï¼ˆå«æ‘˜è¦ï¼‰
ç”ŸæˆåŒ…å«è®ºæ–‡æ‘˜è¦çš„å‘¨æŠ¥ï¼Œæ–¹ä¾¿ç”¨æˆ·é€‰æ‹©æ„Ÿå…´è¶£çš„è®ºæ–‡è¿›è¡Œæ·±å…¥è§£è¯»
"""

import re
import json
import os
from datetime import datetime
from urllib.request import urlopen, Request
from html.parser import HTMLParser

ARXIV_URL = "https://arxiv.org/list/cs/recent"
STATE_FILE = os.path.expanduser("~/.openclaw/workspace/.arxiv-cs-weekly-state.json")

class ArXivParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.papers = []
        self.current_paper = {}
        self.in_item = False
        self.in_title = False
        self.in_authors = False
        self.in_subjects = False
        self.text_buffer = ""
        
    def handle_starttag(self, tag, attrs):
        attrs_dict = dict(attrs)
        if tag == 'a' and 'href' in attrs_dict:
            href = attrs_dict['href']
            if '/abs/' in href:
                arxiv_id = href.split('/abs/')[-1]
                if arxiv_id and not self.in_item:
                    self.in_item = True
                    self.current_paper = {'arxiv_id': arxiv_id}
        
        if tag == 'div' and attrs_dict.get('class') == 'list-title mathjax':
            self.in_title = True
            self.text_buffer = ""
        if tag == 'div' and attrs_dict.get('class') == 'list-authors':
            self.in_authors = True
            self.text_buffer = ""
        if tag == 'div' and attrs_dict.get('class') == 'list-subjects':
            self.in_subjects = True
            self.text_buffer = ""
            
    def handle_endtag(self, tag):
        if self.in_title and tag == 'div':
            self.in_title = False
            self.current_paper['title'] = re.sub(r'^Title:\s*', '', self.text_buffer.strip())
        if self.in_authors and tag == 'div':
            self.in_authors = False
            authors = re.sub(r'^Authors:\s*', '', self.text_buffer.strip())
            self.current_paper['authors'] = re.sub(r'<[^>]+>', '', authors)
        if self.in_subjects and tag == 'div':
            self.in_subjects = False
            self.current_paper['subjects'] = re.sub(r'^Subjects:\s*', '', self.text_buffer.strip())
        if tag == 'dd' and self.in_item and self.current_paper:
            if self.current_paper.get('title'):
                self.papers.append(self.current_paper)
            self.in_item = False
            self.current_paper = {}
            
    def handle_data(self, data):
        if self.in_title or self.in_authors or self.in_subjects:
            self.text_buffer += data

def fetch_page():
    try:
        req = Request(ARXIV_URL, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'})
        with urlopen(req, timeout=30) as response:
            return response.read().decode('utf-8')
    except Exception as e:
        print(f"Error: {e}", file=os.sys.stderr)
        return None

def fetch_abstract(arxiv_id):
    """è·å–å•ç¯‡è®ºæ–‡çš„æ‘˜è¦"""
    try:
        url = f"https://arxiv.org/abs/{arxiv_id}"
        req = Request(url, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'})
        with urlopen(req, timeout=10) as response:
            html = response.read().decode('utf-8')
            # æå–æ‘˜è¦
            match = re.search(r'<blockquote[^>]*class="abstract mathjax"[^>]*>.*?<span[^>]*>Abstract:</span>(.*?)</blockquote>', html, re.DOTALL)
            if match:
                abstract = re.sub(r'<[^>]+>', '', match.group(1))
                abstract = ' '.join(abstract.split())  # æ¸…ç†ç©ºç™½
                return abstract[:500] + "..." if len(abstract) > 500 else abstract
    except Exception as e:
        print(f"è·å–æ‘˜è¦å¤±è´¥ {arxiv_id}: {e}", file=os.sys.stderr)
    return "[æ‘˜è¦è·å–å¤±è´¥]"

def load_state():
    try:
        if os.path.exists(STATE_FILE):
            with open(STATE_FILE, 'r') as f:
                return json.load(f)
    except:
        pass
    return {'last_paper_ids': [], 'last_check_date': None}

def save_state(paper_ids):
    try:
        os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)
        with open(STATE_FILE, 'w') as f:
            json.dump({'last_paper_ids': paper_ids, 'last_check_date': datetime.now().isoformat()}, f)
    except:
        pass

def simplify_subject(text):
    if not text:
        return "CS"
    codes = re.findall(r'\((cs\.[A-Z]+)\)', text)
    if codes:
        mapping = {
            'cs.AI': 'AI', 'cs.CL': 'NLP', 'cs.CV': 'CV',
            'cs.LG': 'ML', 'cs.RO': 'æœºå™¨äºº', 'cs.DB': 'æ•°æ®åº“',
            'cs.SE': 'è½¯ä»¶å·¥ç¨‹', 'cs.CR': 'å®‰å…¨', 'cs.IR': 'IR',
            'cs.MM': 'å¤šåª’ä½“', 'cs.DC': 'åˆ†å¸ƒå¼', 'cs.OS': 'ç³»ç»Ÿ',
            'cs.PL': 'ç¼–ç¨‹è¯­è¨€', 'cs.SC': 'ç§‘å­¦è®¡ç®—'
        }
        names = [mapping.get(c, c) for c in codes[:2]]
        return 'ã€'.join(names)
    return text

def get_keywords(title):
    keywords = []
    t = title.lower()
    maps = {
        'llm': 'LLM', 'language model': 'LLM', 'transformer': 'Transformer',
        'diffusion': 'æ‰©æ•£', 'vision': 'è§†è§‰', 'image': 'å›¾åƒ',
        'video': 'è§†é¢‘', 'robot': 'æœºå™¨äºº', 'reinforcement': 'RL',
        'multimodal': 'å¤šæ¨¡æ€', '3d': '3D', 'optimization': 'ä¼˜åŒ–',
        'attention': 'æ³¨æ„åŠ›', 'embedding': 'Embedding', 'rag': 'RAG',
        'agent': 'Agent', 'prompt': 'Prompt', 'fine-tuning': 'å¾®è°ƒ',
        'zero-shot': 'é›¶æ ·æœ¬', 'chain-of-thought': 'CoT',
        'test-time': 'TTT', 'training': 'è®­ç»ƒ', 'inference': 'æ¨ç†'
    }
    for k, v in maps.items():
        if k in t and v not in keywords:
            keywords.append(v)
    return keywords[:3]

def generate_full_report():
    """ç”Ÿæˆå®Œæ•´æŠ¥å‘Šï¼ŒåŒ…å«æ‘˜è¦"""
    html = fetch_page()
    if not html:
        return None, []
    
    parser = ArXivParser()
    parser.feed(html)
    papers = parser.papers
    
    state = load_state()
    prev_ids = state.get('last_paper_ids', [])
    
    current_ids = [p.get('arxiv_id') for p in papers if p.get('arxiv_id')]
    new_papers = [p for p in papers if p.get('arxiv_id') and p.get('arxiv_id') not in prev_ids]
    
    now = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    lines = [
        f"ğŸ“š **ArXiv CS è®ºæ–‡å‘¨æŠ¥** ({now})",
        "",
        f"æœ¬å‘¨æ–°å¢ *{len(papers)}* ç¯‡è®ºæ–‡ï¼ŒğŸ†• å…¨æ–° *{len(new_papers)}* ç¯‡",
        "",
        "ğŸ’¡ **æç¤º**ï¼šå¦‚å¯¹æŸç¯‡è®ºæ–‡æ„Ÿå…´è¶£ï¼Œå¯å›å¤ \"è§£è¯»ç¬¬Xç¯‡\" æˆ– \"è§£è¯» arXiv:ID\"",
        "",
        "---",
        ""
    ]
    
    # å–å‰10ç¯‡ï¼Œæ¯ç¯‡éƒ½è·å–æ‘˜è¦
    display_count = min(10, len(papers))
    
    for i, p in enumerate(papers[:display_count], 1):
        title = p.get('title', 'æœªçŸ¥')
        authors = p.get('authors', 'æœªçŸ¥')
        aid = p.get('arxiv_id', '')
        subj = simplify_subject(p.get('subjects', ''))
        
        alist = [a.strip() for a in authors.split(',') if a.strip()]
        if len(alist) > 3:
            authors_short = f"{', '.join(alist[:3])} ç­‰{len(alist)}äºº"
        else:
            authors_short = authors
        
        kws = get_keywords(title)
        kw_str = f"ã€Œ{' Â· '.join(kws)}ã€" if kws else ""
        is_new = "ğŸ†• " if aid not in prev_ids else ""
        
        # è·å–æ‘˜è¦
        print(f"æ­£åœ¨è·å–è®ºæ–‡ {i}/{display_count} çš„æ‘˜è¦...", file=os.sys.stderr)
        abstract = fetch_abstract(aid)
        
        lines.append(f"### {i}. {is_new}{title}")
        if kw_str:
            lines.append(kw_str)
        lines.append(f"ğŸ‘¤ {authors_short} | ğŸ·ï¸ {subj} | ğŸ†” {aid}")
        lines.append(f"ğŸ”— https://arxiv.org/abs/{aid}")
        lines.append("")
        lines.append(f"ğŸ“ **æ‘˜è¦**ï¼š{abstract}")
        lines.append("")
        lines.append("---")
        lines.append("")
    
    # å…¶ä»–è®ºæ–‡ç®€å•åˆ—å‡º
    if len(papers) > display_count:
        lines.append("### ğŸ“ æ›´å¤šè®ºæ–‡")
        lines.append("")
        for i, p in enumerate(papers[display_count:display_count+10], display_count+1):
            title = p.get('title', 'æœªçŸ¥')[:50] + "..." if len(p.get('title', '')) > 50 else p.get('title', 'æœªçŸ¥')
            aid = p.get('arxiv_id', '')
            subj = simplify_subject(p.get('subjects', ''))
            is_new = "ğŸ†• " if aid not in prev_ids else ""
            lines.append(f"{i}. {is_new}*{title}* ({subj}) - [arXiv:{aid}](https://arxiv.org/abs/{aid})")
        lines.append("")
        if len(papers) > display_count + 10:
            lines.append(f"...è¿˜æœ‰ {len(papers) - display_count - 10} ç¯‡")
        lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("ğŸ¤– *OpenClaw è‡ªåŠ¨ç”Ÿæˆ* | å›å¤ \"è§£è¯»ç¬¬Nç¯‡\" è·å–æ·±åº¦åˆ†æ")
    
    save_state(current_ids)
    return "\n".join(lines), papers[:display_count]

if __name__ == "__main__":
    report, papers = generate_full_report()
    if report:
        print(report)
        print("\n" + "="*60, file=os.sys.stderr)
        print(f"å‘¨æŠ¥ç”Ÿæˆå®Œæˆï¼å…± {len(papers)} ç¯‡è®ºæ–‡å¸¦æ‘˜è¦", file=os.sys.stderr)
        print("="*60, file=os.sys.stderr)
    else:
        print("è·å–è®ºæ–‡å¤±è´¥")
        exit(1)
